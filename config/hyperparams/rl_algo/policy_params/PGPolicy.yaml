# :param torch.nn.Module model: a model following the rules in :class:`~tianshou.policy.BasePolicy`. (s -> logits)
# :param torch.optim.Optimizer optim: a torch.optim for optimizing the model.
# :param dist_fn: distribution class for computing the action.
# :type dist_fn: Type[torch.distributions.Distribution]

discount_factor: 0.01
# :param float discount_factor: in [0, 1]. Default to 0.99.

action_scaling: False
# :param bool action_scaling: whether to map actions from range [-1, 1] to range
#     [action_spaces.low, action_spaces.high]. Default to True.

reward_normalization: False

action_bound_method: "clip"
# :param str action_bound_method: method to bound action to range [-1, 1], can be
#     either "clip" (for simply clipping the action), "tanh" (for applying tanh
#     squashing) for now, or empty string for no bounding. Default to "clip".

# :param Optional[gym.Space] action_space: env's action space, mandatory if you want
#     to use option "action_scaling" or "action_bound_method". Default to None.

lr_scheduler: null
# :param lr_scheduler: a learning rate scheduler that adjusts the learning rate in
#     optimizer in each policy.update(). Default to None (no lr_scheduler).

deterministic_eval: True
# :param bool deterministic_eval: whether to use deterministic action instead of
#     stochastic action sampled by the policy. Default to False.